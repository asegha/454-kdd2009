---
title: "Model Based EDA"
author: "Jay Swinney"
date: "July 10, 2015"
output: html_document
---

# DONT FORGET TO REMOVE ALL USES OF THE TINY DATA!!!!!!!!!!!!

```{r, message=FALSE, warning=FALSE, echo = FALSE}
library(lattice)
library(plyr)
library(dplyr)
library(tidyr)
library(grid)
library(gridExtra)
library(ROCR)
library(e1071)
library(knitr)
library(ggplot2)


# set document width
# read in the data to R
# I'm using na.stings = '' to replace blanks with na
# this also helps R read the numerical varaibles as numerical
setwd('C:/Users/Jay/Documents/Northwestern/predict_454/KDD_Cup_2009/')
df <- read.csv('orange_small_train.data', header = TRUE,
               sep = '\t', na.strings = '')
# read the target variables
churn_ <- read.csv('orange_small_train_churn.labels', header = FALSE)
appetency_ <- read.csv('orange_small_train_appetency.labels', header = FALSE)
upsell_ <- read.csv('orange_small_train_upselling.labels', header = FALSE)

churn_[churn_$V1 < 0,] <- 0
appetency_[appetency_$V1 < 0,] <- 0
upsell_[upsell_$V1 < 0,] <- 0
```
## Imputing Missing Data

Our strategy to impute missing data is to replace missing numeric values with a 0 and the create a boolean variable that indicates missingness. For categorical variables, all classes that represent less than 1% of the total observations were grouped into an "other" category, then a separate missing class was created.
```{r, message=FALSE, warning=FALSE}

# impute mising data with zeros and "missing"
# also creates missing variable column
for (i in names(df)){
  vclass <- class(df[,i])
  if(vclass == 'logical'){
    # some of the variables are 100% missing, they are the only logical class vars
    # so we can safely remove all logical class vars
    df[,i] <- NULL
  }else if(vclass %in% c('integer', 'numeric')){
    #first check that there are missing variables
    if(sum(is.na(df[,i])) == 0) next
    # create a missing variable column
    df[,paste(i,'_missing',sep='')] <- as.integer(is.na(df[,i]))
    # fill missing variables with 0
    df[is.na(df[,i]),i] <- 0
  }else{
    # gather infrequent levels into 'other'
    levels(df[,i])[xtabs(~df[,i])/dim(df)[1] < 0.015] <- 'other'
    # replace NA with 'missing'
    levels(df[,i]) <- append(levels(df[,i]), 'missing')
    df[is.na(df[,i]), i] <- 'missing'
  }
}
```

```{r, message=FALSE, warning=FALSE, echo = FALSE}
# add the target variables to the data frame
df$churn <- churn_$V1
df$appetency <- appetency_$V1
df$upsell <- upsell_$V1
```


Create testing and training data sets as well as a matrix form of the data that is required by some of the classifiers used in this analysis.
```{r, message=FALSE, warning=FALSE}
# get the index for training/testing data
set.seed(123)
smp_size <- floor(0.75 * nrow(df))
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
# making a "tiny" data set so I cn quickly test r markdown and graphical paramters
# this will be removed in the submitted version
tiny_ind <- sample(seq_len(nrow(df)), size = floor(0.01 * nrow(df)))
# split the data
train <- df[train_ind, ]
test <- df[-train_ind, ]
tiny <- df[tiny_ind, ]

df_mat <- select(df, -churn, -appetency, -upsell)

for (i in names(df_mat)){
  if (class(df_mat[,i]) == 'factor'){
    for(level in unique(df_mat[,i])){
      df_mat[sprintf('%s_dummy_%s', i, level)] <- ifelse(df_mat[,i] == level, 1, 0)
    }
    df_mat[,i] <- NULL
  } else {
    # scale numeric variables
    # this is important for regularized logistic regression and KNN
    df_mat[,i] <- scale(df_mat[,i])
  }
}

df_mat <- data.matrix(df_mat)
```

# Churn

The challenge for the KKD cup 2009 consisted of predicting 3 variables from the same data set. This paper will focus on one variable at a time starting with churn.

## Logistic Regression with Elastic-Net Penalty

A useful technique for understanding which variables have predictive power is to apply logistic regression with a regularization term. In this case elastic-net penalty is used to explore the predictive importance of the variables.

http://www.stanford.edu/~hastie/glmnet/glmnet_alpha.html
```{r lreg_churn, fig.align='center', message=FALSE, warning=FALSE}
library(glmnet)
# regularized logistic regression with cross validation
# this takes a while, try using nfolds < 10 to reduce time
lreg.cv <- cv.glmnet(df_mat[train_ind,], factor(train$churn), family = "binomial",
                     nfolds = 10, type.measure = 'auc')
# view the Area Under the Curve for different values of lambda.
plot(lreg.cv)
title('Cross Validation Curve Logistic Regression',line =+2.8)
```
This plot shows that not all the variables are useful for classification. Two vertical lines in this plot represent the model with the best performance and the most regularized model within one standard deviation of the top performer. Performance is measured on out of sample data. The regularized and cross validated logistic regression selected a model with 155 non-zero variables
<br>

Some of the variables selected by the regularized logistic regression are in the table below with their coefficients. Variables with a coefficient that is very close to zero were left out for brevity.
```{r kable, message=FALSE, warning=FALSE, results= 'asis', echo=FALSE}
cv_coefs <- data.frame(coeficient = coef(lreg.cv)[abs(coef(lreg.cv)) > 1e-3])
row.names(cv_coefs) <- row.names(coef(lreg.cv))[abs(as.vector(coef(lreg.cv))) > 1e-3]
kable(cv_coefs, caption = "Variables Selected by Elastic-Net")
```



```{r, fig.align='center', message=FALSE, warning=FALSE, echo=FALSE, eval=FALSE}
library(FNN)

auc_vec <- rep(0, 20)

for(i in 1:20){
  #print(sprintf('trying k = %d', i))
  yhat <- knn(df_mat[train_ind,], df_mat[-train_ind,],
              cl = factor(train$churn), k = i, prob = TRUE)
  pred <- prediction((as.numeric(yhat[1:dim(df_mat[-train_ind,])[1]]) - 1) * attr(yhat,'prob'),
                     factor(test$churn))
  # the following commented out code is for use with the tiny data set
  # yhat <- knn(df_mat[tiny_ind,], df_mat[tiny_ind,],
  #             cl = factor(tiny$churn), k = i, prob = TRUE)
  # pred <- prediction((as.numeric(yhat[1:dim(df_mat[tiny_ind,])[1]]) - 1) * attr(yhat,'prob'),
  #                    factor(tiny$churn))
  perf <- performance(pred, measure = "tpr", x.measure = "fpr")
  #print(sprintf('AUC: %f',
  #              attributes(performance(pred, 'auc'))$y.values[[1]]))
  auc_vec[i] <- attributes(performance(pred, 'auc'))$y.values[[1]]
}

p <- qplot(y = auc_vec, color = 'AUC') + geom_line() +
            xlab('k = x') + ylab('AUC') + ggtitle('K-NN')
p

# This plot shows that when viewed from an AUC stand point, the k-nearest-neighbors algorithm might as well be random guessing for all values of k. What can be deduced from this is that there are probably not many pockets of similar customers that have churned.

```

## Decision Tree

```{r dt_churn, fig.align='center', message=FALSE, warning=FALSE}
library(rpart)
library(rpart.plot)

my_tree <- rpart(factor(churn)~.,
                 data = select(train, -appetency, -upsell),
                 method = 'class',
                 control=rpart.control(minsplit=40, minbucket=10, cp=0.001))

rpart.plot(my_tree, main = 'Churn Decision Tree')
```
The results from the decision tree give an indication of how useful each of the variables are at predicting churn. This tree is fairly shallow, so any of the variables that made it into the tree will most likely show up in other models that give some indication of variable importance. One interesting thing to not about this tree is that variables 126 and 226 both show up twice in the tree, confirming what has been seen from the logistic regression with elastic net penalty and the random forest variable importance in the next section.

## Random Forest

```{r rf_churn, fig.align='center', message=FALSE, warning=FALSE}
library(randomForest)
set.seed(123)
churn_rf <- randomForest(factor(churn)~.,
                         data = select(train, -appetency, -upsell),
                         ntree = 10, nodesize = 10, importance = TRUE)

varImpPlot(churn_rf, type = 2, main = 'Variable Importance Churn')
```
With the random forest as with the decision tree and logistic regression Var226 has shown to be an important indicator of churn. Variable 204 also shows up high in the variable importance plot from the random forest and in the single decision tree from the previous section.

```{r, fig.align='center', message=FALSE, warning=FALSE}
yhat <- predict(churn_rf, select(test, -appetency, -upsell), type = 'prob')

pred <- prediction(yhat[,2], factor(test$churn))
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
print(sprintf('AUC: %f', attributes(performance(pred, 'auc'))$y.values[[1]]))
plot(perf, col=rainbow(10), main = 'Random Forest ROC Curve')
```
The accuracy of the random forest leaves something to be desired, there is clearly more work to do. It is not displayed here, but the random forest fit extremely well to in-sample data, this indicates that there is more work to be done to combat over-fitting. Options include changing the requirements for leaf and split sizes and trying the random forest with a subset of variables such as the ones selected by regularized logistic regression.


```{r, fig.align='center', message=FALSE, warning=FALSE, echo=FALSE, eval=FALSE}
## Principal Components
pca <- princomp(df_mat)


library(ggbiplot)

p <- ggbiplot(pca, groups = factor(df$churn), ellipse = FALSE,
              var.axes = FALSE) +
  ggtitle('First 2 Principal Components') +
  xlim(-3, 3) + ylim(-3, 3) +
  scale_fill_discrete(name = 'Churn')

p
```

# Appetency
The next response variable to discuss is appetency. As defined in the task description on the KDD website, appetency is the propensity to buy a service or a product.

## Logistic Regression with Elastic-Net Penalty
```{r lreg_app, fig.align='center', message=FALSE, warning=FALSE}
lreg.cv <- cv.glmnet(df_mat[train_ind,], factor(train$appetency), family = "binomial",
                     nfolds = 8, type.measure = 'auc')
# view the bionmial deviance (log loss) of differnt values of lambda
plot(lreg.cv)
title('Cross Validation Curve Logistic Regression', line =+2.8)
```
The results from the logistic regression are very promising and interesting. The AUC peaks above 0.8 which is nice to see, but more interestingly the AUC does not dramatically decline till almost all of the variables are removed from the model. This shows that one or two of the variables are very strong indicators of appetency. <br>

Taking a look at the 3 variables in the highly regularized model (right-most vertical line) shows that Var126 and a certain level of Var218 plus an intercept are very indicative of appetency. This is encouraging because it suggest that predicting appetency will be an easier problem.
```{r kable, message=FALSE, warning=FALSE, results= 'asis', echo=FALSE}
cv_coefs <- data.frame(
  coeficient = coef(lreg.cv, s = 'lambda.1se')[abs(coef(lreg.cv,
                                                        s = 'lambda.1se')) > 1e-3])

row.names(cv_coefs) <- row.names(coef(lreg.cv,
   s = 'lambda.1se'))[abs(as.vector(coef(lreg.cv, s = 'lambda.1se'))) > 1e-3]

kable(cv_coefs, caption = "Variables Selected by Elastic-Net")
```


```{r lreg_app, fig.align='center', message=FALSE, warning=FALSE}
yhat <- predict(lreg.cv, df_mat[-train_ind,], type = 'response')

pred <- prediction(yhat, factor(test$appetency))
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
print(sprintf('AUC for Regularized Logistic Regression: %f',
              attributes(performance(pred, 'auc'))$y.values[[1]]))
plot(perf, col=rainbow(10), main = 'ROC Curve Logistic Regression')
```
As shown by this ROC curve constructed on out of sample data, the logistic regression performs very well identifying appetency.


## Decision Tree

```{r dt_churn, fig.align='center', message=FALSE, warning=FALSE}
my_tree <- rpart(factor(appetency)~.,
                 data = select(train, -churn, -upsell),
                 method = 'class',
                 control=rpart.control(minsplit=40, minbucket=10, cp=0.001))

rpart.plot(my_tree, main = 'Appetency Decision Tree')
my_tree
```

The Decision Tree classifier selected 7 variables as the most predictive variables.  The 7 variables are listed below with the highest predictive value listed first.<br>
The following configuration: minsplit=40 to set the minimum number of observations per node, minbucket=10 to set the minimum number of total nodes, and cp=0.001 to set the cost complexity factor with a split that must decrease the overall lack of fit by a factor of 0.001.<br>
1.	Var126 <br>
2.	Var218 <br>
3.	Var204 <br>
4.	Var38 <br>
5.	Var206 <br>
6.	Var223 <br>
7.	Var81 <br>

## Random Forest

```{r rf_churn, fig.align='center', message=FALSE, warning=FALSE}
appetency_rf <- randomForest(factor(appetency)~.,
                         data = select(train, -churn, -upsell),
                         ntree = 10, nodesize = 10, importance = TRUE)

varImpPlot(appetency_rf, type = 2, main = 'Variable Importance Appetency')
```
An interesting take away from this plot is that the random forest identified variable 226 & 126 as two of the top three most important variables. This echos the output from the logistic regression and further confirms that these are important variables. However it will be shown that the random forest did not perform nearly as well as the regularized logistic regression, this is because the random forest is severely over-fit, it will need significant tuning before it is on par with the regularized logistic regression.


```{r rf_churn, fig.align='center', message=FALSE, warning=FALSE}
yhat <- predict(appetency_rf, select(test, -churn, -upsell), type = 'prob')

pred <- prediction(yhat[,2], factor(test$appetency))
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
print(sprintf('AUC: %f', attributes(performance(pred, 'auc'))$y.values[[1]]))
plot(perf, col=rainbow(10))
```


# Up-Sell
The last response variable to analyze is up-sell. As defined in the task description, up-selling can imply selling something additional, or selling something that is more profitable or otherwise preferable for the seller instead of the original sale.

## Logistic Regression with Elastic-Net Penalty
```{r lreg_upsell, fig.align='center', message=FALSE, warning=FALSE}
lreg.cv_upsell <- cv.glmnet(df_mat[train_ind,], factor(train$upsell), family = "binomial",
                     nfolds = 8, type.measure = 'auc')
# view the bionmial deviance (log loss) of differnt values of lambda
plot(lreg.cv_upsell)
title('Cross Validation Curve Logistic Regression',line =+2.8)
```
The results from the regularized logistic regression are both promising and somewhat disappointing at the same time. The plot shows that comparable performance can be achieved by removing all but about 80 variables, but unfortunately the regularization does not appear to yield much in the way of performance gain.


```{r lreg_upsell, fig.align='center', message=FALSE, warning=FALSE, echo = FALSE, eval=FALSE}
cv_coefs <- data.frame( coeficient = coef(lreg.cv_upsell, s = 'lambda.1se')[
  abs(coef(lreg.cv_upsell, s = 'lambda.1se')) > 1e-3])

row.names(cv_coefs) <- row.names(coef(lreg.cv_upsell, s = 'lambda.1se'))[
  abs(as.vector(coef(lreg.cv_upsell, s = 'lambda.1se'))) > 1e-3]
```

## Decision Tree
```{r dt_upsell, fig.align='center', message=FALSE, warning=FALSE}
my_tree <- rpart(factor(upsell)~.,
                 data = select(train, -appetency, -churn),
                 method = 'class',
                 control=rpart.control(minsplit=100, minbucket=10, cp=0.001))

rpart.plot(my_tree, main = 'Up-Sell Decision Tree')
```

A number of control options  were used for rpart() -, namely minsplit to set the minimum number of observations per node, minbucket - minimum number of total nodes , cp - split must decrease the overall lack of fit by a factor of 0.001 (cost complexity factor). It was observed that Var 126 and Var 28 were chosen and always had a high importance.These variables probably have good predictive value for up-sell.

```{r, message = FALSE}
summary(my_tree)
```

## Random Forest
```{r rf_upsell, fig.align='center', message=FALSE, warning=FALSE}
upsell_rf <- randomForest(factor(upsell)~.,
                         data = select(train, -appetency, -churn),
                         ntree = 10, nodesize = 10, importance = TRUE)

varImpPlot(upsell_rf, type = 2, main = 'Variable Importance Up-Sell')
```

The Random Forest classifier selects close to 200 predictor variables as having significant predictive value for up-sell. This is a very large number.  We do see that the mean decrease in Gini Index is highest on including Var 126. This matches with the results from Decision tree that Var 126 has a higher predictor value.

# Other Analysis
In addition to the steps listed here, KNN and PCA was used to explore all of the response varaibles without any interesting results. Before any model fitting was done, each predictor variable was examined individually to look for patterns and structure manually. The uni-variate work is not included here becuase it is too space consumptive.
